{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478efc38-ac99-40a7-9e13-b72840f14e19",
   "metadata": {},
   "source": [
    "# Distributed debate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c5593-c810-4c93-90de-b2c389b878ab",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This example simulates a debate competition with three participant agents, including the affirmative side (Pro), the negative side (Con), and the adjudicator (Judge). \n",
    "\n",
    "Pro believes that AGI can be achieved using the GPT model framework, while Con contests it. Judge listens to both sides' arguments and provides an analytical judgment on which side presented a more compelling and reasonable case.\n",
    "\n",
    "A fully distributed version can be found in `examples/distributed/distributed_debate.py`.\n",
    "Here we provide a standalone multi-process version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e5966-752c-4a28-b63e-3239008d6b3a",
   "metadata": {},
   "source": [
    "To install AgentScope, please follow the steps in [README.md](../README.md#installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97a3fc-6bed-4a0f-bf61-e977630a159c",
   "metadata": {},
   "source": [
    "First, we need to set the model configs of AgentScope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072fc64",
   "metadata": {},
   "source": [
    "Second, let's start the three agents in the debate. Note that each agent here will automatically starts a sub-process, and the `reply` method is executed within the sub-process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260aab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agentscope\n",
    "from agentscope.agents.dialog_agent import DialogAgent\n",
    "\n",
    "agentscope.init(model_configs=\"./model_configs.json\")\n",
    "\n",
    "pro_agent = DialogAgent(\n",
    "    name=\"Pro\",\n",
    "    model_config_name=\"qwen\",\n",
    "    use_memory=True,\n",
    "    sys_prompt=\"Assume the role of a debater who is arguing in favor of the proposition that AGI (Artificial General Intelligence) can be achieved using the GPT model framework. Construct a coherent and persuasive argument, including scientific, technological, and theoretical evidence, to support the statement that GPT models are a viable path to AGI. Highlight the advancements in language understanding, adaptability, and scalability of GPT models as key factors in progressing towards AGI.\",\n",
    ").to_dist()\n",
    "con_agent = DialogAgent(\n",
    "    name=\"Con\",\n",
    "    model_config_name=\"qwen\",\n",
    "    use_memory=True,\n",
    "    sys_prompt=\"Assume the role of a debater who is arguing against the proposition that AGI can be achieved using the GPT model framework. Construct a coherent and persuasive argument, including scientific, technological, and theoretical evidence, to support the statement that GPT models, while impressive, are insufficient for reaching AGI. Discuss the limitations of GPT models such as lack of understanding, consciousness, ethical reasoning, and general problem-solving abilities that are essential for true AGI.\",\n",
    ").to_dist()\n",
    "judge_agent = DialogAgent(\n",
    "    name=\"Judge\",\n",
    "    model_config_name=\"qwen\",\n",
    "    use_memory=True,\n",
    "    sys_prompt=\"Assume the role of an impartial judge in a debate where the affirmative side argues that AGI can be achieved using the GPT model framework, and the negative side contests this. Listen to both sides' arguments and provide an analytical judgment on which side presented a more compelling and reasonable case. Consider the strength of the evidence, the persuasiveness of the reasoning, and the overall coherence of the arguments presented by each side.\"\n",
    ").to_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8024-fa7e-4d7f-bf35-a78511a47ab3",
   "metadata": {},
   "source": [
    "Next, write the main debate competition process.\n",
    "Note that we need to use `msghub` to ensure each agent in the debate knows the speeaches of all other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6391fb00-f74c-42c5-b742-56b7a773f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-22 14:01:09.411 | INFO     | agentscope.models:read_model_configs:171 - Load configs for model wrapper: gemini, qwen, deepseek, moonshot\n",
      "2024-03-22 14:01:09.412 | INFO     | agentscope.utils.monitor:_create_monitor_table:341 - Init [monitor_metrics] as the monitor table\n",
      "2024-03-22 14:01:09.412 | INFO     | agentscope.utils.monitor:_create_monitor_table:342 - Init [monitor_metrics_quota_exceeded] as the monitor trigger\n",
      "2024-03-22 14:01:09.413 | INFO     | agentscope.utils.monitor:__init__:311 - SqliteMonitor initialization completed at [./runs/run_20240322-140101_w632l9/agentscope.db]\n",
      "2024-03-22 14:01:09.413 | INFO     | agentscope.models.model:__init__:256 - Initialize model [qwen]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zz/anaconda3/envs/agentscope/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/zz/anaconda3/envs/agentscope/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/zz/code/agentscope/src/agentscope/agents/rpc_agent.py\", line 213, in setup_rcp_agent_server\n",
      "    servicer = RpcServerSideWrapper(\n",
      "  File \"/Users/zz/code/agentscope/src/agentscope/agents/rpc_agent.py\", line 463, in __init__\n",
      "    self.result_pool = ExpiringDict(\n",
      "TypeError: 'NoneType' object is not callable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m hint \u001b[38;5;241m=\u001b[39m Msg(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39mANNOUNCEMENT)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m msghub(participants\u001b[38;5;241m=\u001b[39mparticipants, announcement\u001b[38;5;241m=\u001b[39mhint):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     26\u001b[0m         pro_resp \u001b[38;5;241m=\u001b[39m pro_agent(x)\n",
      "File \u001b[0;32m~/code/agentscope/src/agentscope/msghub.py:48\u001b[0m, in \u001b[0;36mMsgHubManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannouncement \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparticipants:\n\u001b[0;32m---> 48\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannouncement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/code/agentscope/src/agentscope/agents/rpc_agent.py:147\u001b[0m, in \u001b[0;36mRpcAgent.observe\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Union[\u001b[38;5;28mdict\u001b[39m, Sequence[\u001b[38;5;28mdict\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcall_func(\n\u001b[1;32m    149\u001b[0m         func_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_observe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m         value\u001b[38;5;241m=\u001b[39mserialize(x),  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/code/agentscope/src/agentscope/agents/rpc_agent.py:130\u001b[0m, in \u001b[0;36mRpcAgent._launch_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_launch_server\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Launch a rpc server and update the port and the client\"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_launcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_launcher\u001b[38;5;241m.\u001b[39mport\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m RpcAgentClient(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "File \u001b[0;32m~/code/agentscope/src/agentscope/agents/rpc_agent.py:408\u001b[0m, in \u001b[0;36mRpcAgentServerLauncher.launch\u001b[0;34m(self, in_subprocess)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"launch a rpc agent server.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m        input, such as UserAgent, please set this value to False.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_subprocess:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_in_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch_in_main()\n",
      "File \u001b[0;32m~/code/agentscope/src/agentscope/agents/rpc_agent.py:391\u001b[0m, in \u001b[0;36mRpcAgentServerLauncher._launch_in_sub\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m server_process \u001b[38;5;241m=\u001b[39m Process(\n\u001b[1;32m    374\u001b[0m     target\u001b[38;5;241m=\u001b[39msetup_rcp_agent_server,\n\u001b[1;32m    375\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     },\n\u001b[1;32m    389\u001b[0m )\n\u001b[1;32m    390\u001b[0m server_process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_con\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m start_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver \u001b[38;5;241m=\u001b[39m server_process\n",
      "File \u001b[0;32m~/anaconda3/envs/agentscope/lib/python3.9/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/anaconda3/envs/agentscope/lib/python3.9/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/agentscope/lib/python3.9/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agentscope.msghub import msghub\n",
    "from agentscope.message import Msg\n",
    "from agentscope.utils.logging_utils import logger\n",
    "\n",
    "# Rules explained before the debate begins \n",
    "ANNOUNCEMENT = \"\"\"\n",
    "Welcome to the debate on whether Artificial General Intelligence (AGI) can be achieved using the GPT model framework. This debate will consist of three rounds. In each round, the affirmative side will present their argument first, followed by the negative side. After both sides have presented, the adjudicator will summarize the key points and analyze the strengths of the arguments.\n",
    "\n",
    "The rules are as follows:\n",
    "\n",
    "Each side must present clear, concise arguments backed by evidence and logical reasoning.\n",
    "No side may interrupt the other while they are presenting their case.\n",
    "After both sides have presented, the adjudicator will have time to deliberate and will then provide a summary, highlighting the most persuasive points from both sides.\n",
    "The adjudicator's summary will not declare a winner for the individual rounds but will focus on the quality and persuasiveness of the arguments.\n",
    "At the conclusion of the three rounds, the adjudicator will declare the overall winner based on which side won two out of the three rounds, considering the consistency and strength of the arguments throughout the debate.\n",
    "Let us begin the first round. The affirmative side: please present your argument for why AGI can be achieved using the GPT model framework.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Setup the main debate competition process\"\"\"\n",
    "participants = [pro_agent, con_agent, judge_agent]\n",
    "hint = Msg(name=\"System\", content=ANNOUNCEMENT)\n",
    "x = None\n",
    "with msghub(participants=participants, announcement=hint):\n",
    "    for _ in range(3):\n",
    "        pro_resp = pro_agent(x)\n",
    "        logger.chat(pro_resp)\n",
    "        con_resp = con_agent(pro_resp)\n",
    "        logger.chat(con_resp)\n",
    "        x = judge_agent(con_resp)\n",
    "        logger.chat(x)\n",
    "    x = judge_agent(x)\n",
    "    logger.chat(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc5033",
   "metadata": {},
   "source": [
    "Finally, just wait for the above code to run and watch the debate proceed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
